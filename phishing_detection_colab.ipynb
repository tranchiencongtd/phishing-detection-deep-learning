{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49a76b0a",
   "metadata": {},
   "source": [
    "## üì¶ B∆∞·ªõc 1: C√†i ƒë·∫∑t th∆∞ vi·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dcc568",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.15.0 scikit-learn pandas numpy matplotlib seaborn -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfd06d3",
   "metadata": {},
   "source": [
    "## üìÇ B∆∞·ªõc 2: Clone repository ho·∫∑c upload files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57c9145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Clone t·ª´ GitHub (n·∫øu c√≥)\n",
    "# !git clone https://github.com/your-username/phishing-detection.git\n",
    "# %cd phishing-detection\n",
    "\n",
    "# Option 2: Upload files t·ª´ m√°y t√≠nh\n",
    "# Click v√†o bi·ªÉu t∆∞·ª£ng folder b√™n tr√°i -> Upload files\n",
    "# Upload to√†n b·ªô project structure\n",
    "\n",
    "# Check current directory\n",
    "!pwd\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbeab5e",
   "metadata": {},
   "source": [
    "## üìä B∆∞·ªõc 3: Import libraries v√† setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e9ee2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import json\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Check GPU\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f72ad4a",
   "metadata": {},
   "source": [
    "## üèóÔ∏è B∆∞·ªõc 4: Define Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f16d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# ============================================================================\n",
    "# Attention Layer\n",
    "# ============================================================================\n",
    "class SeqSelfAttention(layers.Layer):\n",
    "    \"\"\"Sequence Self-Attention Layer\"\"\"\n",
    "    \n",
    "    def __init__(self, attention_dim=128, **kwargs):\n",
    "        super(SeqSelfAttention, self).__init__(**kwargs)\n",
    "        self.attention_dim = attention_dim\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(\n",
    "            name='attention_weight',\n",
    "            shape=(input_shape[-1], self.attention_dim),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            name='attention_bias',\n",
    "            shape=(self.attention_dim,),\n",
    "            initializer='zeros',\n",
    "            trainable=True\n",
    "        )\n",
    "        self.u = self.add_weight(\n",
    "            name='attention_context',\n",
    "            shape=(self.attention_dim,),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        super(SeqSelfAttention, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        uit = tf.tanh(tf.tensordot(inputs, self.W, axes=1) + self.b)\n",
    "        ait = tf.tensordot(uit, self.u, axes=1)\n",
    "        attention_weights = tf.nn.softmax(ait, axis=1)\n",
    "        attention_weights = tf.expand_dims(attention_weights, axis=-1)\n",
    "        weighted_input = inputs * attention_weights\n",
    "        return weighted_input\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'attention_dim': self.attention_dim})\n",
    "        return config\n",
    "\n",
    "print(\"‚úì Attention layer defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ce00fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1. ANN Model\n",
    "# ============================================================================\n",
    "class ANNModel(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, dropout=0.3, **kwargs):\n",
    "        super(ANNModel, self).__init__(**kwargs)\n",
    "        self.embedding = layers.Embedding(vocab_size, embedding_dim, mask_zero=False)\n",
    "        self.dense1 = layers.Dense(hidden_dim, activation='relu')\n",
    "        self.dropout1 = layers.Dropout(dropout)\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense2 = layers.Dense(128, activation='relu')\n",
    "        self.dropout2 = layers.Dropout(dropout)\n",
    "        self.output_layer = layers.Dense(1, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# ============================================================================\n",
    "# 2. ATT Model\n",
    "# ============================================================================\n",
    "class ATTModel(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, attention_dim=128, dropout=0.3, **kwargs):\n",
    "        super(ATTModel, self).__init__(**kwargs)\n",
    "        self.embedding = layers.Embedding(vocab_size, embedding_dim, mask_zero=False)\n",
    "        self.attention = SeqSelfAttention(attention_dim)\n",
    "        self.dropout1 = layers.Dropout(dropout)\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense1 = layers.Dense(128, activation='relu')\n",
    "        self.dropout2 = layers.Dropout(dropout)\n",
    "        self.output_layer = layers.Dense(1, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.attention(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# ============================================================================\n",
    "# 3. RNN Model\n",
    "# ============================================================================\n",
    "class RNNModel(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, lstm_units=256, dropout=0.3, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.embedding = layers.Embedding(vocab_size, embedding_dim, mask_zero=False)\n",
    "        self.lstm = layers.LSTM(lstm_units, return_sequences=False)\n",
    "        self.dropout1 = layers.Dropout(dropout)\n",
    "        self.dense1 = layers.Dense(128, activation='relu')\n",
    "        self.dropout2 = layers.Dropout(dropout)\n",
    "        self.output_layer = layers.Dense(1, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.lstm(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# ============================================================================\n",
    "# 4. BRNN Model\n",
    "# ============================================================================\n",
    "class BRNNModel(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, lstm_units=256, dropout=0.3, **kwargs):\n",
    "        super(BRNNModel, self).__init__(**kwargs)\n",
    "        self.embedding = layers.Embedding(vocab_size, embedding_dim, mask_zero=False)\n",
    "        self.bidirectional_lstm = layers.Bidirectional(\n",
    "            layers.LSTM(lstm_units, return_sequences=False)\n",
    "        )\n",
    "        self.dropout1 = layers.Dropout(dropout)\n",
    "        self.dense1 = layers.Dense(128, activation='relu')\n",
    "        self.dropout2 = layers.Dropout(dropout)\n",
    "        self.output_layer = layers.Dense(1, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.bidirectional_lstm(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# ============================================================================\n",
    "# 5. CNN Model\n",
    "# ============================================================================\n",
    "class CNNModel(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, num_filters=256, kernel_size=3, dropout=0.3, **kwargs):\n",
    "        super(CNNModel, self).__init__(**kwargs)\n",
    "        self.embedding = layers.Embedding(vocab_size, embedding_dim, mask_zero=False)\n",
    "        self.conv1d = layers.Conv1D(num_filters, kernel_size, activation='relu', padding='same')\n",
    "        self.pooling = layers.GlobalMaxPooling1D()\n",
    "        self.dropout1 = layers.Dropout(dropout)\n",
    "        self.dense1 = layers.Dense(128, activation='relu')\n",
    "        self.dropout2 = layers.Dropout(dropout)\n",
    "        self.output_layer = layers.Dense(1, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.conv1d(x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "print(\"‚úì All 5 models defined: ANN, ATT, RNN, BRNN, CNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e291c413",
   "metadata": {},
   "source": [
    "## üì• B∆∞·ªõc 5: Load Data\n",
    "\n",
    "**L∆∞u √Ω**: B·∫°n c·∫ßn upload c√°c file sau v√†o Colab:\n",
    "- `data/datasets/char_X_train.npy`\n",
    "- `data/datasets/char_X_val.npy`\n",
    "- `data/datasets/char_X_test.npy`\n",
    "- `data/datasets/char_y_train.npy`\n",
    "- `data/datasets/char_y_val.npy`\n",
    "- `data/datasets/char_y_test.npy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445e0876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory if not exists\n",
    "!mkdir -p data/datasets\n",
    "\n",
    "# Load preprocessed data\n",
    "data_dir = Path(\"data/datasets\")\n",
    "\n",
    "X_train = np.load(data_dir / 'char_X_train.npy')\n",
    "y_train = np.load(data_dir / 'char_y_train.npy')\n",
    "X_val = np.load(data_dir / 'char_X_val.npy')\n",
    "y_val = np.load(data_dir / 'char_y_val.npy')\n",
    "X_test = np.load(data_dir / 'char_X_test.npy')\n",
    "y_test = np.load(data_dir / 'char_y_test.npy')\n",
    "\n",
    "print(f\"Train: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Val:   X={X_val.shape}, y={y_val.shape}\")\n",
    "print(f\"Test:  X={X_test.shape}, y={y_test.shape}\")\n",
    "\n",
    "# Calculate vocab size\n",
    "vocab_size = int(X_train.max()) + 1\n",
    "print(f\"\\nVocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d86929",
   "metadata": {},
   "source": [
    "## üèãÔ∏è B∆∞·ªõc 6: Build v√† Compile Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682fcbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configurations\n",
    "configs = {\n",
    "    'ann': {\n",
    "        'vocab_size': vocab_size,\n",
    "        'embedding_dim': 128,\n",
    "        'hidden_dim': 256,\n",
    "        'dropout': 0.3\n",
    "    },\n",
    "    'att': {\n",
    "        'vocab_size': vocab_size,\n",
    "        'embedding_dim': 128,\n",
    "        'attention_dim': 128,\n",
    "        'dropout': 0.3\n",
    "    },\n",
    "    'rnn': {\n",
    "        'vocab_size': vocab_size,\n",
    "        'embedding_dim': 128,\n",
    "        'lstm_units': 256,\n",
    "        'dropout': 0.3\n",
    "    },\n",
    "    'brnn': {\n",
    "        'vocab_size': vocab_size,\n",
    "        'embedding_dim': 128,\n",
    "        'lstm_units': 256,\n",
    "        'dropout': 0.3\n",
    "    },\n",
    "    'cnn': {\n",
    "        'vocab_size': vocab_size,\n",
    "        'embedding_dim': 128,\n",
    "        'num_filters': 256,\n",
    "        'kernel_size': 3,\n",
    "        'dropout': 0.3\n",
    "    }\n",
    "}\n",
    "\n",
    "# Build models\n",
    "models = {}\n",
    "models['ann'] = ANNModel(**configs['ann'])\n",
    "models['att'] = ATTModel(**configs['att'])\n",
    "models['rnn'] = RNNModel(**configs['rnn'])\n",
    "models['brnn'] = BRNNModel(**configs['brnn'])\n",
    "models['cnn'] = CNNModel(**configs['cnn'])\n",
    "\n",
    "# Compile models\n",
    "for model_name, model in models.items():\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            keras.metrics.Precision(name='precision'),\n",
    "            keras.metrics.Recall(name='recall'),\n",
    "            keras.metrics.AUC(name='auc')\n",
    "        ]\n",
    "    )\n",
    "    print(f\"‚úì {model_name.upper()} model built and compiled\")\n",
    "\n",
    "print(\"\\n‚úì All models ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa65f68",
   "metadata": {},
   "source": [
    "## üéØ B∆∞·ªõc 7: Train Models\n",
    "\n",
    "Ch·ªçn model mu·ªën train (ho·∫∑c train t·∫•t c·∫£)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9143c4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Select which models to train (set to True/False)\n",
    "train_config = {\n",
    "    'ann': True,\n",
    "    'att': True,\n",
    "    'rnn': True,\n",
    "    'brnn': True,\n",
    "    'cnn': True\n",
    "}\n",
    "\n",
    "# Store training histories\n",
    "histories = {}\n",
    "\n",
    "# Train selected models\n",
    "for model_name, should_train in train_config.items():\n",
    "    if not should_train:\n",
    "        print(f\"‚è≠Ô∏è  Skipping {model_name.upper()}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üöÄ Training {model_name.upper()} Model\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train\n",
    "    history = models[model_name].fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    histories[model_name] = history\n",
    "    print(f\"\\n‚úÖ {model_name.upper()} training completed!\")\n",
    "\n",
    "print(\"\\nüéâ All training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f675a16",
   "metadata": {},
   "source": [
    "## üìä B∆∞·ªõc 8: Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3adcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all trained models\n",
    "results = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä EVALUATING ALL MODELS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    if model_name not in histories:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nüîç Evaluating {model_name.upper()}...\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_acc, test_precision, test_recall, test_auc = model.evaluate(\n",
    "        X_test, y_test, verbose=0\n",
    "    )\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_proba = model.predict(X_test, verbose=0)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate F1 Score\n",
    "    from sklearn.metrics import f1_score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results[model_name] = {\n",
    "        'accuracy': test_acc,\n",
    "        'precision': test_precision,\n",
    "        'recall': test_recall,\n",
    "        'f1_score': f1,\n",
    "        'auc': test_auc,\n",
    "        'loss': test_loss\n",
    "    }\n",
    "    \n",
    "    print(f\"Accuracy:  {test_acc:.4f}\")\n",
    "    print(f\"Precision: {test_precision:.4f}\")\n",
    "    print(f\"Recall:    {test_recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(f\"AUC:       {test_auc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Evaluation completed!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68db0452",
   "metadata": {},
   "source": [
    "## üìà B∆∞·ªõc 9: Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dea5852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "if results:\n",
    "    # Create comparison DataFrame\n",
    "    df_results = pd.DataFrame(results).T\n",
    "    df_results = df_results.sort_values('accuracy', ascending=False)\n",
    "    \n",
    "    print(\"\\nüìä Model Comparison:\")\n",
    "    print(df_results.to_string())\n",
    "    \n",
    "    # Plot metrics comparison\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('Model Comparison - All Metrics', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'auc', 'loss']\n",
    "    \n",
    "    for idx, metric in enumerate(metrics):\n",
    "        ax = axes[idx // 3, idx % 3]\n",
    "        data = df_results[metric].sort_values(ascending=(metric != 'loss'))\n",
    "        \n",
    "        bars = ax.barh(data.index, data.values)\n",
    "        \n",
    "        # Color bars\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, len(data)))\n",
    "        for bar, color in zip(bars, colors):\n",
    "            bar.set_color(color)\n",
    "        \n",
    "        ax.set_xlabel(metric.replace('_', ' ').title())\n",
    "        ax.set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, v in enumerate(data.values):\n",
    "            ax.text(v, i, f' {v:.4f}', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find best model\n",
    "    best_model = df_results['accuracy'].idxmax()\n",
    "    print(f\"\\nüèÜ Best Model: {best_model.upper()}\")\n",
    "    print(f\"   Accuracy: {df_results.loc[best_model, 'accuracy']:.4f}\")\n",
    "    print(f\"   F1 Score: {df_results.loc[best_model, 'f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72468a4",
   "metadata": {},
   "source": [
    "## üìâ B∆∞·ªõc 10: Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3c0b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history for each model\n",
    "for model_name, history in histories.items():\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    fig.suptitle(f'{model_name.upper()} Training History', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Plot accuracy\n",
    "    axes[0].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "    axes[0].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].set_title('Model Accuracy')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Plot loss\n",
    "    axes[1].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "    axes[1].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].set_title('Model Loss')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25559f71",
   "metadata": {},
   "source": [
    "## üîç B∆∞·ªõc 11: Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a6fcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix for each model\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Confusion Matrices - All Models', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, (model_name, model) in enumerate(models.items()):\n",
    "    if model_name not in histories:\n",
    "        continue\n",
    "    \n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_proba = model.predict(X_test, verbose=0)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Plot\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Legitimate', 'Phishing'],\n",
    "                yticklabels=['Legitimate', 'Phishing'])\n",
    "    ax.set_title(f'{model_name.upper()}')\n",
    "    ax.set_ylabel('True Label')\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "\n",
    "# Remove empty subplot\n",
    "if len(histories) < 6:\n",
    "    fig.delaxes(axes[1, 2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86be5d5",
   "metadata": {},
   "source": [
    "## üíæ B∆∞·ªõc 12: Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe8ec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained models\n",
    "!mkdir -p trained_models\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    if model_name not in histories:\n",
    "        continue\n",
    "    \n",
    "    save_path = f\"trained_models/{model_name}_model.h5\"\n",
    "    model.save(save_path)\n",
    "    print(f\"‚úì Saved {model_name.upper()} to {save_path}\")\n",
    "\n",
    "# Save results to CSV\n",
    "if results:\n",
    "    df_results.to_csv('model_comparison_results.csv')\n",
    "    print(\"\\n‚úì Results saved to model_comparison_results.csv\")\n",
    "\n",
    "print(\"\\n‚úÖ All models saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a338c3",
   "metadata": {},
   "source": [
    "## üéØ B∆∞·ªõc 13: Test Prediction (Optional)\n",
    "\n",
    "Test v·ªõi m·ªôt v√†i URL m·∫´u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6eccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple character tokenization for testing\n",
    "def simple_tokenize(url, max_len=200):\n",
    "    \"\"\"Simple character tokenization\"\"\"\n",
    "    # Convert to character indices (a=1, b=2, etc.)\n",
    "    chars = list(url.lower())\n",
    "    indices = [ord(c) for c in chars]\n",
    "    \n",
    "    # Pad or truncate\n",
    "    if len(indices) < max_len:\n",
    "        indices = indices + [0] * (max_len - len(indices))\n",
    "    else:\n",
    "        indices = indices[:max_len]\n",
    "    \n",
    "    return np.array([indices])\n",
    "\n",
    "# Test URLs\n",
    "test_urls = [\n",
    "    \"https://www.google.com\",\n",
    "    \"http://suspicious-site-login-verify.tk\",\n",
    "    \"https://github.com\",\n",
    "]\n",
    "\n",
    "print(\"\\nüîç Testing Predictions:\\n\")\n",
    "\n",
    "for url in test_urls:\n",
    "    print(f\"URL: {url}\")\n",
    "    \n",
    "    # Tokenize\n",
    "    X = simple_tokenize(url)\n",
    "    \n",
    "    # Predict with all models\n",
    "    print(\"Predictions:\")\n",
    "    for model_name, model in models.items():\n",
    "        if model_name not in histories:\n",
    "            continue\n",
    "        \n",
    "        pred_proba = model.predict(X, verbose=0)[0][0]\n",
    "        pred_label = \"Phishing\" if pred_proba > 0.5 else \"Legitimate\"\n",
    "        print(f\"  {model_name.upper():6s}: {pred_proba:.4f} ‚Üí {pred_label}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Note: N√†y ch·ªâ l√† demo ƒë∆°n gi·∫£n. ƒê·ªÉ c√≥ k·∫øt qu·∫£ ch√≠nh x√°c, c·∫ßn d√πng tokenizer ƒë√£ train.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42b5668",
   "metadata": {},
   "source": [
    "## üì• B∆∞·ªõc 14: Download Models (Optional)\n",
    "\n",
    "Download c√°c model ƒë√£ train v·ªÅ m√°y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7873c1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip all trained models\n",
    "!zip -r trained_models.zip trained_models/\n",
    "\n",
    "# Download\n",
    "from google.colab import files\n",
    "files.download('trained_models.zip')\n",
    "files.download('model_comparison_results.csv')\n",
    "\n",
    "print(\"‚úÖ Files ready for download!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14daca5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù T√≥m t·∫Øt\n",
    "\n",
    "Notebook n√†y train v√† so s√°nh 5 models Deep Learning:\n",
    "\n",
    "1. **ANN** - Simple feed-forward network\n",
    "2. **ATT** - Attention-based network\n",
    "3. **RNN** - LSTM network\n",
    "4. **BRNN** - Bidirectional LSTM\n",
    "5. **CNN** - Convolutional network\n",
    "\n",
    "### K·∫øt qu·∫£:\n",
    "- Models ƒë∆∞·ª£c train v√† evaluate tr√™n test set\n",
    "- So s√°nh performance c·ªßa t·∫•t c·∫£ models\n",
    "- Visualization: training history, metrics, confusion matrix\n",
    "- Models ƒë∆∞·ª£c save ƒë·ªÉ s·ª≠ d·ª•ng sau\n",
    "\n",
    "### ƒê·ªÉ s·ª≠ d·ª•ng:\n",
    "1. Upload data files (.npy)\n",
    "2. Run t·∫•t c·∫£ cells theo th·ª© t·ª±\n",
    "3. Xem k·∫øt qu·∫£ comparison\n",
    "4. Download models ƒë√£ train\n",
    "\n",
    "üéâ **Happy Training!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
